{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shianchamian/Web/blob/main/lab_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPGYECGb98jE",
        "outputId": "7ede47bb-dcd1-42c2-d630-02dede9fccd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "# importing of modules for CIFAR-10 CNN\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, Activation, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# importing of service libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwplhjOF-Mej",
        "outputId": "881be658-e085-4dd2-e0e1-9aa4a0c9c917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "input_X_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "#load dataset\n",
        "(input_X_train, output_y_train), (input_X_test, output_y_test) = cifar100.load_data()\n",
        "input_X_train = input_X_train.astype('float32') / 255.\n",
        "input_X_test = input_X_test.astype('float32') / 255.\n",
        "output_y_train = to_categorical(output_y_train, 100)\n",
        "output_y_test = to_categorical(output_y_test, 100)\n",
        "\n",
        "print('input_X_train shape:', input_X_train.shape)\n",
        "print(input_X_train.shape[0], 'train samples')\n",
        "print(input_X_test.shape[0], 'test samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LE91X6XTVwr2"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter options\n",
        "# batch_sizes = [256]\n",
        "# dence_sizes = [512]\n",
        "\n",
        "learning_rates = [0.001]\n",
        "\n",
        "# Generate all combinations of hyperparameters (grid search)\n",
        "param_grid = list(learning_rates)\n",
        "\n",
        "# Store results from all experiments\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6_Oy1M4uof87"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "def save_training_plot(history, lr, folder=\"figures\"):\n",
        "    \"\"\"Save accuracy and loss plots for a training run.\"\"\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
        "\n",
        "    # Accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Train')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend(loc='lower right')\n",
        "\n",
        "    # Loss\n",
        "    ax2.plot(history.history['loss'], label='Train')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    filename = f\"acc_loss__lr{lr}.png\"\n",
        "    fig_path = os.path.join(folder, filename)\n",
        "    plt.savefig(fig_path)\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved training plot: {fig_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7C_lG8CXMn1",
        "outputId": "830b0dad-5e2f-4286-ba46-47797f85ba01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Running: lr=0.001\n",
            "Epoch 1/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.0324 - loss: 4.5414 - val_accuracy: 0.1039 - val_loss: 3.9225\n",
            "Epoch 2/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.0969 - loss: 3.8694 - val_accuracy: 0.1211 - val_loss: 3.7231\n",
            "Epoch 3/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1360 - loss: 3.5578 - val_accuracy: 0.1658 - val_loss: 3.4517\n",
            "Epoch 4/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1872 - loss: 3.3039 - val_accuracy: 0.2190 - val_loss: 3.1670\n",
            "Epoch 5/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.2260 - loss: 3.0597 - val_accuracy: 0.2679 - val_loss: 2.9215\n",
            "Epoch 6/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.2599 - loss: 2.8935 - val_accuracy: 0.3139 - val_loss: 2.7415\n",
            "Epoch 7/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.2889 - loss: 2.7627 - val_accuracy: 0.3339 - val_loss: 2.6613\n",
            "Epoch 8/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3124 - loss: 2.6616 - val_accuracy: 0.3564 - val_loss: 2.4760\n",
            "Epoch 9/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3305 - loss: 2.5416 - val_accuracy: 0.3026 - val_loss: 2.7999\n",
            "Epoch 10/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3436 - loss: 2.4915 - val_accuracy: 0.3675 - val_loss: 2.4948\n",
            "Epoch 11/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3610 - loss: 2.4191 - val_accuracy: 0.3859 - val_loss: 2.3643\n",
            "Epoch 12/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3673 - loss: 2.3551 - val_accuracy: 0.4013 - val_loss: 2.3117\n",
            "Epoch 13/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3870 - loss: 2.2878 - val_accuracy: 0.4188 - val_loss: 2.2272\n",
            "Epoch 14/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3970 - loss: 2.2513 - val_accuracy: 0.4060 - val_loss: 2.2891\n",
            "Epoch 15/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4055 - loss: 2.1896 - val_accuracy: 0.4298 - val_loss: 2.2025\n",
            "Epoch 16/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4190 - loss: 2.1533 - val_accuracy: 0.4381 - val_loss: 2.1802\n",
            "Epoch 17/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4170 - loss: 2.1406 - val_accuracy: 0.4229 - val_loss: 2.1797\n",
            "Epoch 18/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4328 - loss: 2.0874 - val_accuracy: 0.4689 - val_loss: 2.0067\n",
            "Epoch 19/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4405 - loss: 2.0504 - val_accuracy: 0.4423 - val_loss: 2.1077\n",
            "Epoch 20/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.4502 - loss: 2.0166 - val_accuracy: 0.4454 - val_loss: 2.0894\n",
            "Epoch 21/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4565 - loss: 1.9852 - val_accuracy: 0.4803 - val_loss: 1.9521\n",
            "Epoch 22/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4640 - loss: 1.9468 - val_accuracy: 0.4436 - val_loss: 2.1374\n",
            "Epoch 23/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4614 - loss: 1.9467 - val_accuracy: 0.4502 - val_loss: 2.0716\n",
            "Epoch 24/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4836 - loss: 1.8869 - val_accuracy: 0.4840 - val_loss: 1.9184\n",
            "Epoch 25/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4772 - loss: 1.8906 - val_accuracy: 0.4916 - val_loss: 1.9136\n",
            "Epoch 26/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4839 - loss: 1.8571 - val_accuracy: 0.4820 - val_loss: 1.9505\n",
            "Epoch 27/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4896 - loss: 1.8555 - val_accuracy: 0.4871 - val_loss: 1.8987\n",
            "Epoch 28/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4957 - loss: 1.8170 - val_accuracy: 0.4911 - val_loss: 1.8821\n",
            "Epoch 29/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4973 - loss: 1.8072 - val_accuracy: 0.5107 - val_loss: 1.8250\n",
            "Epoch 30/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5034 - loss: 1.7869 - val_accuracy: 0.4908 - val_loss: 1.9046\n",
            "Epoch 31/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5045 - loss: 1.7746 - val_accuracy: 0.4916 - val_loss: 1.8904\n",
            "Epoch 32/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5134 - loss: 1.7278 - val_accuracy: 0.5144 - val_loss: 1.7899\n",
            "Epoch 33/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5173 - loss: 1.7216 - val_accuracy: 0.5106 - val_loss: 1.7977\n",
            "Epoch 34/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5147 - loss: 1.7420 - val_accuracy: 0.5097 - val_loss: 1.8290\n",
            "Epoch 35/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5224 - loss: 1.7034 - val_accuracy: 0.4912 - val_loss: 1.9017\n",
            "Epoch 36/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5287 - loss: 1.6988 - val_accuracy: 0.5085 - val_loss: 1.8164\n",
            "Epoch 37/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5231 - loss: 1.6965 - val_accuracy: 0.5171 - val_loss: 1.8011\n",
            "Epoch 38/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5265 - loss: 1.6793 - val_accuracy: 0.4990 - val_loss: 1.8549\n",
            "Epoch 39/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5382 - loss: 1.6525 - val_accuracy: 0.5140 - val_loss: 1.7935\n",
            "Epoch 40/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5355 - loss: 1.6426 - val_accuracy: 0.5193 - val_loss: 1.7604\n",
            "Epoch 41/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5329 - loss: 1.6593 - val_accuracy: 0.5237 - val_loss: 1.7597\n",
            "Epoch 42/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5407 - loss: 1.6292 - val_accuracy: 0.5170 - val_loss: 1.7803\n",
            "Epoch 43/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5461 - loss: 1.6020 - val_accuracy: 0.5158 - val_loss: 1.7957\n",
            "Epoch 44/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5444 - loss: 1.6080 - val_accuracy: 0.5098 - val_loss: 1.8007\n",
            "Epoch 45/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5520 - loss: 1.5761 - val_accuracy: 0.5268 - val_loss: 1.7509\n",
            "Epoch 46/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5528 - loss: 1.5902 - val_accuracy: 0.5377 - val_loss: 1.7228\n",
            "Epoch 47/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5525 - loss: 1.5703 - val_accuracy: 0.5131 - val_loss: 1.8062\n",
            "Epoch 48/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5513 - loss: 1.5814 - val_accuracy: 0.5166 - val_loss: 1.8137\n",
            "Epoch 49/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5604 - loss: 1.5638 - val_accuracy: 0.5134 - val_loss: 1.8287\n",
            "Epoch 50/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5532 - loss: 1.5703 - val_accuracy: 0.5305 - val_loss: 1.7281\n",
            "Epoch 51/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5604 - loss: 1.5558 - val_accuracy: 0.5225 - val_loss: 1.7610\n",
            "Epoch 52/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5564 - loss: 1.5412 - val_accuracy: 0.5306 - val_loss: 1.7236\n",
            "Epoch 53/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5612 - loss: 1.5351 - val_accuracy: 0.5345 - val_loss: 1.7309\n",
            "Epoch 54/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5682 - loss: 1.5254 - val_accuracy: 0.5165 - val_loss: 1.8148\n",
            "Epoch 55/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5727 - loss: 1.5010 - val_accuracy: 0.5042 - val_loss: 1.8609\n",
            "Epoch 56/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5738 - loss: 1.5122 - val_accuracy: 0.5301 - val_loss: 1.7368\n",
            "Epoch 57/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5681 - loss: 1.5183 - val_accuracy: 0.5225 - val_loss: 1.7577\n",
            "Epoch 58/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5745 - loss: 1.5016 - val_accuracy: 0.5154 - val_loss: 1.8121\n",
            "Epoch 59/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5737 - loss: 1.4838 - val_accuracy: 0.5331 - val_loss: 1.7367\n",
            "Epoch 60/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5788 - loss: 1.4860 - val_accuracy: 0.5207 - val_loss: 1.7831\n",
            "Epoch 61/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5804 - loss: 1.4709 - val_accuracy: 0.5364 - val_loss: 1.7454\n",
            "Epoch 62/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5772 - loss: 1.4768 - val_accuracy: 0.5250 - val_loss: 1.7705\n",
            "Epoch 63/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5794 - loss: 1.4754 - val_accuracy: 0.5359 - val_loss: 1.7132\n",
            "Epoch 64/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5822 - loss: 1.4614 - val_accuracy: 0.5227 - val_loss: 1.7891\n",
            "Epoch 65/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5834 - loss: 1.4595 - val_accuracy: 0.5413 - val_loss: 1.7043\n",
            "Epoch 66/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5814 - loss: 1.4467 - val_accuracy: 0.5358 - val_loss: 1.7411\n",
            "Epoch 67/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5812 - loss: 1.4641 - val_accuracy: 0.5419 - val_loss: 1.7080\n",
            "Epoch 68/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5897 - loss: 1.4330 - val_accuracy: 0.5279 - val_loss: 1.7624\n",
            "Epoch 69/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5903 - loss: 1.4178 - val_accuracy: 0.5325 - val_loss: 1.7444\n",
            "Epoch 70/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5893 - loss: 1.4345 - val_accuracy: 0.5360 - val_loss: 1.7406\n",
            "Epoch 71/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5893 - loss: 1.4201 - val_accuracy: 0.5415 - val_loss: 1.7120\n",
            "Epoch 72/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5893 - loss: 1.4330 - val_accuracy: 0.5222 - val_loss: 1.8208\n",
            "Epoch 73/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5989 - loss: 1.3968 - val_accuracy: 0.5461 - val_loss: 1.6703\n",
            "Epoch 74/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5926 - loss: 1.4128 - val_accuracy: 0.5422 - val_loss: 1.7226\n",
            "Epoch 75/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5994 - loss: 1.3999 - val_accuracy: 0.5420 - val_loss: 1.6991\n",
            "Epoch 76/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6037 - loss: 1.3826 - val_accuracy: 0.5256 - val_loss: 1.7632\n",
            "Epoch 77/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5966 - loss: 1.4041 - val_accuracy: 0.5431 - val_loss: 1.7121\n",
            "Epoch 78/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6002 - loss: 1.3918 - val_accuracy: 0.5457 - val_loss: 1.6889\n",
            "Epoch 79/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5950 - loss: 1.3995 - val_accuracy: 0.5385 - val_loss: 1.7295\n",
            "Epoch 80/80\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6019 - loss: 1.3908 - val_accuracy: 0.5418 - val_loss: 1.7108\n",
            "Saved training plot: figures/acc_loss__lr0.001.png\n",
            " Final val_acc = 0.5418\n",
            "\n",
            " All experiments completed. Results saved to results.csv\n"
          ]
        }
      ],
      "source": [
        "# Start the experiment\n",
        "for  lr in param_grid:\n",
        "      print(f\"\\n Running: lr={lr}\")\n",
        "\n",
        "      # Build the model\n",
        "      model = Sequential([\n",
        "          Input(shape=(32, 32, 3)),\n",
        "          Conv2D(32, 3, padding='same'),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "          Conv2D(32, 3),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "          MaxPooling2D(pool_size=(2, 2)),\n",
        "          Dropout(0.25),\n",
        "\n",
        "          Conv2D(64, 3, padding='same'),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "\n",
        "          Conv2D(64, 3),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "          MaxPooling2D(pool_size=(2, 2)),\n",
        "          Dropout(0.25),\n",
        "\n",
        "          Conv2D(128, 3, padding='same'),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "\n",
        "\n",
        "          Conv2D(128, 3),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "          Dropout(0.25),\n",
        "\n",
        "          Conv2D(128, 3),\n",
        "          BatchNormalization(),\n",
        "          Activation('relu'),\n",
        "          Dropout(0.25),\n",
        "\n",
        "          MaxPooling2D(pool_size=(2, 2)),\n",
        "          Dropout(0.25),\n",
        "\n",
        "          Flatten(),\n",
        "          Dense(256),\n",
        "          Activation('relu'),\n",
        "          Dropout(0.5),\n",
        "          Dense(100, activation='softmax')\n",
        "      ])\n",
        "\n",
        "      # Compile the model\n",
        "      model.compile(\n",
        "          optimizer=Adam(learning_rate=lr),\n",
        "          loss='categorical_crossentropy',\n",
        "          metrics=['accuracy']\n",
        "      )\n",
        "\n",
        "      # Train the model\n",
        "      history = model.fit(\n",
        "          input_X_train, output_y_train,\n",
        "          epochs=80,\n",
        "          batch_size=64,\n",
        "          validation_split=0.2,\n",
        "          verbose=1  # Set to 1 to display training progress\n",
        "      )\n",
        "\n",
        "\n",
        "      save_training_plot(history, lr)\n",
        "\n",
        "\n",
        "      # Record final validation accuracy\n",
        "      final_val_acc = history.history['val_accuracy'][-1]\n",
        "      print(f\" Final val_acc = {final_val_acc:.4f}\")\n",
        "\n",
        "      # Save results to list\n",
        "      results.append({\n",
        "          'batch_size': 64,\n",
        "          'dropout': 0.5,\n",
        "          'val_accuracy': final_val_acc,\n",
        "          'epoch': 80\n",
        "      })\n",
        "\n",
        "# Save results to CSV\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"cifar100_experiment_results.csv\", index=False)\n",
        "print(\"\\n All experiments completed. Results saved to results.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNEMon425/BZavv1DBWE7Eb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}